{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chzqn3xE6bDZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "input_folder = \"/content/gdrive/My Drive/\" #@param {type:\"string\"}\n",
        "#@markdown - Folder containing query structure files\n",
        "output_path = \"/content/gdrive/My Drive/RESULTS.tsv\" #@param {type:\"string\"}\n",
        "#@markdown - Output path & name; results will be written into a .tsv file\n",
        "#@markdown - Additional COMPLETE file will be written containing complete record of all hits\n",
        "continuous_saving_mode = True #@param {type:\"boolean\"}\n",
        "#@markdown - Appends results after each prediction\n",
        "\n",
        "files = []\n",
        "for file in glob.glob(f\"/{input_folder.strip('/')}/*.pdb\"):\n",
        "  files.append(file)\n",
        "\n",
        "print(f\"Collected {len(files)} pdb files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Ch6SDWJl4L",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title FoldSeek\n",
        "from collections import Counter\n",
        "import requests as rq\n",
        "import time\n",
        "import os\n",
        "from IPython.display import Javascript\n",
        "\n",
        "#limits the output window size\n",
        "display(Javascript('google.colab.output.setIframeHeight(250)'))\n",
        "#Prevents socket error\n",
        "time.sleep(5)\n",
        "\n",
        "#pLDDT calculator for AlphaFold models\n",
        "def get_pLDDT_AlphaFold(file):\n",
        "\tresidues_n = int()\n",
        "\tpLDDT_total = float()\n",
        "\twith open(file, \"r\") as f:\n",
        "\t\tcurrent_residue = 0\n",
        "\t\tfor l in f:\n",
        "\n",
        "\t\t\tif l.startswith(\"TER\"):\n",
        "\t\t\t\tcurrent_residue = 0\n",
        "\n",
        "\t\t\tif l.startswith(\"ATOM\") and len(l.split()) == 12:\n",
        "\t\t\t\tif int(l.split()[5]) > current_residue:\n",
        "\t\t\t\t\tcurrent_residue += 1\n",
        "\t\t\t\t\tresidues_n += 1\n",
        "\t\t\t\t\tpLDDT_total += float(l.split()[-2])\n",
        "\n",
        "\t\t\telif l.startswith(\"ATOM\") and len(l.split()) == 11:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tif int(l.split()[4][1:]) > current_residue:\n",
        "\t\t\t\t\t\tcurrent_residue += 1\n",
        "\t\t\t\t\t\tresidues_n += 1\n",
        "\t\t\t\t\t\tpLDDT_total += float(l.split()[-2])\n",
        "\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tif int(l.split()[5]) > current_residue:\n",
        "\t\t\t\t\t\tcurrent_residue += 1\n",
        "\t\t\t\t\t\tresidues_n += 1\n",
        "\t\t\t\t\t\tpLDDT_total += float(l.split()[-2])\n",
        "\n",
        "\tif residues_n == 0:\n",
        "\t\treturn 0\n",
        "\telse:\n",
        "\t\tpLDDT_result = pLDDT_total/residues_n\n",
        "\t\tif pLDDT_result < 1:\n",
        "\t\t\treturn pLDDT_result * 100\n",
        "\t\telse:\n",
        "\t\t\treturn pLDDT_result\n",
        "\n",
        "#pLDDT calculator for ESMFold structures\n",
        "def get_pLDDT_ESMFold(file):\n",
        "\tresidues_n = int()\n",
        "\tpLDDT_total = float()\n",
        "\twith open(file, \"r\") as f:\n",
        "\t\tcurrent_residue = 0\n",
        "\t\tcurrent_residue_atoms = int()\n",
        "\t\tcurrent_residue_pLDDT = int()\n",
        "\t\tfor l in f:\n",
        "\n",
        "\t\t\tif l.startswith(\"TER\"):\n",
        "\n",
        "\t\t\t\tpLDDT_total += current_residue_pLDDT/current_residue_atoms\n",
        "\n",
        "\t\t\tif l.startswith(\"ATOM\"):\n",
        "\n",
        "\t\t\t\tif int(l.split()[5]) > current_residue:\n",
        "\t\t\t\t\tresidues_n += 1\n",
        "\t\t\t\t\tif current_residue_atoms != 0:\n",
        "\t\t\t\t\t\tpLDDT_total += current_residue_pLDDT/current_residue_atoms\n",
        "\t\t\t\t\tcurrent_residue += 1\n",
        "\t\t\t\t\tcurrent_residue_atoms = 1\n",
        "\n",
        "\t\t\t\t\tpLDDT_value = float(l.split()[-2])\n",
        "\t\t\t\t\tif pLDDT_value < 1.01:\n",
        "\t\t\t\t\t\tpLDDT_value = pLDDT_value * 100\n",
        "\t\t\t\t\tcurrent_residue_pLDDT = pLDDT_value\n",
        "\n",
        "\t\t\t\telif int(l.split()[5]) == current_residue:\n",
        "\t\t\t\t\tcurrent_residue_atoms += 1\n",
        "\t\t\t\t\tpLDDT_value = float(l.split()[-2])\n",
        "\t\t\t\t\tif pLDDT_value < 1.01:\n",
        "\t\t\t\t\t\tpLDDT_value = pLDDT_value * 100\n",
        "\t\t\t\t\tcurrent_residue_pLDDT += pLDDT_value\n",
        "\n",
        "\tif residues_n == 0:\n",
        "\t\treturn 0\n",
        "\telse:\n",
        "\t\treturn (pLDDT_total/residues_n)\n",
        "\n",
        "#Calculates the pLDDT of a query structure if possible\n",
        "def calculate_pLDDT(file):\n",
        "\n",
        "  pLDDT = \"No info\"\n",
        "\n",
        "  try:\n",
        "    pLDDT = f\"{get_pLDDT_AlphaFold(file):.2f}\"\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  if pLDDT == \"No info\":\n",
        "    try:\n",
        "      pLDDT = f\"{get_pLDDT_ESMFold(file):.2f}\"\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return pLDDT\n",
        "\n",
        "#Atempts to retrieve UniProt annotation if available\n",
        "def get_Uniprot_annotation(file):\n",
        "\n",
        "  #Removes the prefix and sufix of files from AlphaFold DB\n",
        "  if \"AF-\" in file and \"-F1-model_v\" in file:\n",
        "    seq_ID = file.split(\"/\")[-1][3:-16]\n",
        "  else:\n",
        "    seq_ID = file.split(\"/\")[-1][:-4]\n",
        "\n",
        "  fasta = rq.get(f\"https://rest.uniprot.org/uniprotkb/{seq_ID}.fasta\").text\n",
        "  if fasta.startswith(\">\"):\n",
        "    UniProt_annotation = \" \".join(fasta.split()[1:])\n",
        "    UniProt_annotation = UniProt_annotation[:UniProt_annotation.find(\"OS=\")].strip()\n",
        "\n",
        "    return UniProt_annotation\n",
        "\n",
        "  #If  sequence ID hasn't been found in UniProtKB, returns 'Unknown'\n",
        "  else:\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "#Returns a list of annotations for FoldSeek hits\n",
        "#Filteres out unannotated proteins and hits under the probability threshold\n",
        "def get_annotation_list(file):\n",
        "\n",
        "  probability_threshold = \"0.500\" #@param {type:\"string\"}\n",
        "  #@markdown - Hits above threshold (0-1) will be included in the results\n",
        "  #probability_threshold = \"0.500\"\n",
        "  probability_threshold = float(probability_threshold)\n",
        "\n",
        "  annotations = []\n",
        "  annotations_IDs = []\n",
        "\n",
        "  with open(file, \"r\") as f:\n",
        "    for l in f:\n",
        "      result_ID = l.split(\"\\t\")[1].split(\"-\")[1]\n",
        "      annotation = \" \".join(l.split(\"\\t\")[1].split()[1:])\n",
        "      probability = float(l.split(\"\\t\")[10])\n",
        "      if not \"uncharacterized\" in annotation.lower() and not \"hypothetical\" in annotation.lower() and not \"putative\" in annotation.lower() and not \"predicted\" in annotation.lower() and probability >= probability_threshold:\n",
        "        annotations.append(annotation)\n",
        "        annotations_IDs.append(result_ID)\n",
        "\n",
        "  return annotations, annotations_IDs\n",
        "\n",
        "#Returns top FoldSeek hits for SwissProt, UniProt and AlphaFold_Proteome databases\n",
        "#Returns most common hit annotation across all three databases and the number of occurences\n",
        "def get_hits():\n",
        "\n",
        "  DBs=[\"alis_afdb-swissprot.m8\", \"alis_afdb50.m8\", \"alis_afdb-proteome.m8\"]\n",
        "  annotations = []\n",
        "  best_hits = {}\n",
        "  complete_annotations = []\n",
        "\n",
        "  #Retrieves best hits and creates list of all results\n",
        "  for DB in DBs:\n",
        "    new_DB, new_DB_IDs = get_annotation_list(DB)\n",
        "    if len(new_DB) != 0:\n",
        "      best_hits[DB] = new_DB[0]\n",
        "\n",
        "      complete_DB_records = \"\"\n",
        "      for i in range(len(new_DB)):\n",
        "        complete_DB_records += new_DB_IDs[i] + \" \" + new_DB[i] + \";\"\n",
        "\n",
        "    else:\n",
        "      best_hits[DB] = \"None\"\n",
        "      complete_DB_records = \"None\"\n",
        "    annotations.extend(new_DB)\n",
        "    complete_annotations.append(complete_DB_records.strip(\";\"))\n",
        "\n",
        "  #Retrieves the most common annotation among all hits\n",
        "  most_common = Counter(annotations).most_common(1)\n",
        "  keys_and_values = [(key, value) for key, value in most_common]\n",
        "  if len(keys_and_values) != 0:\n",
        "    most_common, number_of_hits = keys_and_values[0]\n",
        "  else:\n",
        "    most_common = \"None\"\n",
        "    number_of_hits = 0\n",
        "\n",
        "  #Appends all the results to a list\n",
        "  results = []\n",
        "  complete_results = []\n",
        "  for DB in best_hits:\n",
        "    results.append(best_hits[DB])\n",
        "  results.append(most_common)\n",
        "  results.append(str(number_of_hits))\n",
        "\n",
        "  return results, complete_annotations\n",
        "\n",
        "#Cleans the temporary files\n",
        "def clean_tmp():\n",
        "  os.system(\"rm /content/alis_*.m8\")\n",
        "  os.system(\"rm ticket.txt\")\n",
        "  os.system(\"rm Search_Output.gz\")\n",
        "\n",
        "#Uses the FoldSeek API and unzipps the results\n",
        "#API call sometimes fail and produces corrupted .gz file!\n",
        "def FoldSeek_call(file):\n",
        "\n",
        "  print(f\"{file} being predicted..\")\n",
        "\n",
        "  os.system(f\"curl -X POST -F q=@{file} -F 'mode=3diaa' -F 'database[]=afdb50' -F 'database[]=afdb-swissprot' -F 'database[]=afdb-proteome' https://search.foldseek.com/api/ticket > /content/ticket.txt\")\n",
        "\n",
        "  #Retrieves unique FoldSeek run ID\n",
        "  with open(\"/content/ticket.txt\", \"r\") as f:\n",
        "    try:\n",
        "      ticket_id = f.read().split('\"')[3]\n",
        "    except IndexError:\n",
        "      if \"ticket_id\" in locals():\n",
        "        del ticket_id\n",
        "      print(f\"{file} prediction has failed.. \")\n",
        "\n",
        "  if \"ticket_id\" in locals():\n",
        "    #Retrieves FoldSeek results\n",
        "    os.system(f\"curl -X GET https://search.foldseek.com/api/result/download/{ticket_id} --output /content/Search_Output.gz\")\n",
        "    os.system(f'tar -xvzf \"/content/Search_Output.gz\"')\n",
        "\n",
        "#Calls for the FoldSeek search, retrieves UniProt annotation and returns complete list of all annotation details\n",
        "def FoldSeek(file):\n",
        "\n",
        "  annotation = []\n",
        "\n",
        "  #Gets sequence ID\n",
        "  seq_ID = file.split(\"/\")[-1][:-4]\n",
        "  annotation.append(seq_ID)\n",
        "\n",
        "  #Calculates pLDDT\n",
        "  pLDDT = calculate_pLDDT(file)\n",
        "  annotation.append(pLDDT)\n",
        "\n",
        "  #Retrieves UniProt annotation if possible\n",
        "  UniProt_annotation = get_Uniprot_annotation(file)\n",
        "  annotation.append(UniProt_annotation)\n",
        "\n",
        "  #API calls until *.m8 results are produced\n",
        "  while True:\n",
        "    if not os.path.exists(\"/content/alis_afdb-swissprot.m8\") and not os.path.exists(\"/content/alis_afdb-proteome.m8\") and not os.path.exists(\"/content/alis_afdb50.m8\"):\n",
        "      FoldSeek_call(file)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  #Retrieves the FoldSeek annotations\n",
        "  FoldSeek_results, complete_results = get_hits()\n",
        "  annotation.extend(FoldSeek_results)\n",
        "\n",
        "  clean_tmp()\n",
        "\n",
        "  return annotation, complete_results\n",
        "\n",
        "#Returns a string containing the results in TSV format\n",
        "def make_table(rows_list):\n",
        "\n",
        "  tsv = \"\"\n",
        "\n",
        "  for row in rows_list:\n",
        "    tsv += \"\\t\".join(row) + \"\\n\"\n",
        "\n",
        "  return tsv\n",
        "\n",
        "#Main\n",
        "table_rows = [[\"Structure ID\", \"pLDDT\",\"UniProt annotation\", \"Swiss-Prot\", \"UniProt\", \"AlphaFold-Proteomes\",\"Most frequent hit\", \"Most frequent hit n\"]]\n",
        "complete_table_rows = [[\"Structure ID\", \"pLDDT\", \"UniProt annotations\", \"Swiss-Prot\", \"UniProt\", \"AlphaFold-Proteomes\"]]\n",
        "\n",
        "#Writes results after all the computations have been done\n",
        "if continuous_saving_mode == False:\n",
        "  for file in files:\n",
        "\n",
        "    #Google Colab removes the white space\n",
        "    file = file.replace(\"My Drive\", \"MyDrive\")\n",
        "\n",
        "    annotation, complete_annotation = FoldSeek(file)\n",
        "    table_rows.append(annotation)\n",
        "    complete_annotation.insert(0, annotation[2])\n",
        "    complete_annotation.insert(0, annotation[1])\n",
        "    complete_annotation.insert(0, annotation[0])\n",
        "    complete_table_rows.append(complete_annotation)\n",
        "    print(f\"Prediction has been finished for {file}.. \")\n",
        "\n",
        "  results_file = make_table(table_rows)\n",
        "  complete_results_file = make_table(complete_table_rows)\n",
        "\n",
        "  #Writes results into the output folder\n",
        "  with open(f\"/{output_path.strip('/')}\", \"w\") as f:\n",
        "    f.write(results_file)\n",
        "\n",
        "  complete_results_path = output_path.strip(\"/\").split(\".\")[0] + \"_COMPLETE.\" + output_path.strip(\"/\").split(\".\")[1]\n",
        "  with open(f\"/{complete_results_path}\", \"w\") as f:\n",
        "    f.write(complete_results_file)\n",
        "\n",
        "#Continuously appends new rows to the result file\n",
        "else:\n",
        "\n",
        "  #Sets up the result files and writes the header\n",
        "  with open(f\"/{output_path.strip('/')}\", \"w\") as f:\n",
        "    f.write(\"\\t\".join(table_rows[0]) + \"\\n\")\n",
        "\n",
        "  complete_results_path = output_path.strip(\"/\").split(\".\")[0] + \"_COMPLETE.\" + output_path.strip(\"/\").split(\".\")[1]\n",
        "  with open(f\"/{complete_results_path}\", \"w\") as f:\n",
        "    f.write(\"\\t\".join(complete_table_rows[0]) + \"\\n\")\n",
        "  for file in files:\n",
        "\n",
        "    #Google Colab removes the white space\n",
        "    file = file.replace(\"My Drive\", \"MyDrive\")\n",
        "    annotation, complete_annotation = FoldSeek(file)\n",
        "\n",
        "    with open(f\"/{output_path.strip('/')}\", \"a\") as f:\n",
        "      f.write(\"\\t\".join(annotation) + \"\\n\")\n",
        "    with open(f\"/{complete_results_path}\", \"a\") as f:\n",
        "      f.write(\"\\t\".join(annotation[:3]) + \"\\t\" + \"\\t\".join(complete_annotation) + \"\\n\")\n",
        "\n",
        "    print(f\"Prediction has been finished for {file}.. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OYRo1gOXsnhh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
